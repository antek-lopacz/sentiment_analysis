{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not in venv:\n",
    "pip install huggingface-hub==0.26.2\n",
    "\n",
    "In venv:\n",
    "pip install numpy==1.26.4 scipy==1.13.1 gensim==4.3.3 pandas==2.2.3 nltk==3.9.1 transformers==4.46.2\n",
    "\n",
    "- cpu version\n",
    "pip3 install torch torchvision torchaudio\n",
    "- gpu version\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model naming convention: \n",
    "`<EmbeddingType>_<ArchitectureType>_<LayerConfig>_<Activation>_<Extras>_<Task>`\n",
    "\n",
    "#### Example Components\n",
    "\n",
    "- **Embedding Type**: Specify the input representation.  \n",
    "  Examples: `W2V` for Word2Vec, `BERT`, `GloVe`, `TF-IDF`, etc.\n",
    "\n",
    "- **Architecture Type**: Include the model type.  \n",
    "  Examples: `NN`, `LSTM`, `GRU`, `TF` (Transformer), etc.\n",
    "\n",
    "- **Layer Configuration**: Use layer sizes or count.  \n",
    "  Examples: `128-64-32` for layer sizes or `3L` for 3 layers.\n",
    "\n",
    "- **Activation Function**: Specify the activation function.  \n",
    "  Examples: `ReLU`, `LeakyReLU`, `Tanh`, etc.\n",
    "\n",
    "- **Extras**: Include regularization, dropout, or batch normalization (if relevant).  \n",
    "  Examples: `DO30` for 30% dropout, `BN` for BatchNorm.\n",
    "\n",
    "- **Task**: Add a suffix to describe the task.  \n",
    "  Examples: `MC` for multi-class classification, `SC` for single-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPD - libraries, packages, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv_lic\\Scripts\\activate\n",
    "# venv_lic\\Scripts\\deactivate\n",
    "\n",
    "# venv_pc\\Scripts\\activate\n",
    "# venv_pc\\Scripts\\deactivate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# PyTorch info\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# 'word2vec-ruscorpora-300'\n",
    "# 'word2vec-google-news-300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the embeddings\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec checking\n",
    "\n",
    "# word2vec.most_similar('coin')\n",
    "# word2vec.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "# word2vec.most_similar(positive=['swim', 'basketball'], negative=['pool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.environ['USERPROFILE'], r\"OneDrive - SGH\\1. SGH\\Praca licencjacka\\files\\data\")\n",
    "data = pd.read_csv(rf'{data_path}\\reddit_sentiment_august2021.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = data[['body', 'BERT-Sentiment']].head(5)\n",
    "# test_sentence = test_data.iloc[0]['body']\n",
    "reddit_data = data[['body', 'BERT-Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sentiment_label column (binary column based on 'BERT-Sentiment' column)\n",
    "# 'BERT-Sentiment' column: from 1 (negative) to 5 (positive).\n",
    "# Label 0: Positive\n",
    "# Label 1: Neutral\n",
    "# Label 2: Negative\n",
    "class_dict = class_dict = {0: \"Positive\", 1: \"Neutral\", 2: \"Negative\"}\n",
    "\n",
    "# Check for unmatched values\n",
    "unmatched_values = reddit_data[~reddit_data['BERT-Sentiment'].isin([1, 2, 3, 4, 5])]\n",
    "if unmatched_values.empty:\n",
    "    print(\"All good\")\n",
    "\n",
    "# Assign sentiment_label\n",
    "reddit_data['sentiment_label'] = reddit_data['BERT-Sentiment'].apply(\n",
    "    lambda x: 0 if x in [4, 5] else 1 if x == 3 else 2\n",
    ")\n",
    "\n",
    "# Create np array with labels\n",
    "labels = np.array(reddit_data['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Tokenization and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize_function(text, bert_tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenizes input text using a BERT tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str or List[str]): The text or list of sentences to tokenize.\n",
    "        bert_tokenizer (transformers.BertTokenizer): A pre-trained BERT tokenizer.\n",
    "        max_length (int): The maximum length of the tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with tokenized input (`input_ids`) and attention mask (`attention_mask`).\n",
    "    \"\"\"\n",
    "    return bert_tokenizer(\n",
    "        text,\n",
    "        padding=True,             # Add padding to match max_length\n",
    "        truncation=True,          # Truncate sequences longer than max_length\n",
    "        max_length=max_length,    # Define the maximum sequence length\n",
    "        return_tensors=\"pt\"       # Return as PyTorch tensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002700-\\U000027BF\"  # dingbats\n",
    "        \"\\U00002600-\\U000026FF\"  # miscellaneous symbols\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # symbols and pictographs extended-A\n",
    "        \"\\U00002500-\\U00002BEF\"  # chinese symbols\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_body(body, emoji_pattern=emoji_pattern):\n",
    "    body_clean = body.lower() # Lowercase\n",
    "    body_clean = emoji_pattern.sub('', body_clean) # Remove emojis\n",
    "    return body_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens, emoji_pattern=emoji_pattern):\n",
    "    '''\n",
    "    Cleaning tokens, removing ...\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Clean tokens\n",
    "    tokens = [token.lower() for token in tokens]  # Lowercase\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]  # Remove punctuation\n",
    "    tokens = [re.sub(r'http\\S+|www.\\S+', '<URL>', token) for token in tokens]  # Replace URLs\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove non-alphanumeric\n",
    "    tokens = [emoji_pattern.sub('', token) for token in tokens]  # Remove emojis\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization\n",
    "    # rule based\n",
    "reddit_data['clean_tokens'] = reddit_data['body'].apply(lambda x: clean_tokens(tokenize_function(x)))\n",
    "\n",
    "    # cleaning for later BERT tokenizer\n",
    "reddit_data['clean_body'] = reddit_data['body'].apply(lambda x: clean_body(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embedding from words embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector sentence representation\n",
    "# output: 2D np array, shape is (300, num_sentences)\n",
    "\n",
    "def sentence_to_embedding(tokens, word2vec, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens into a single sentence embedding by averaging word vectors.\n",
    "    \"\"\"\n",
    "    embeddings = [\n",
    "        word2vec[token] for token in tokens if token in word2vec.key_to_index\n",
    "    ]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)  # Average word vectors\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)  # Return zero vector if no valid tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to generate sentence embeddings\n",
    "reddit_data['sentence_embedding'] = reddit_data['clean_tokens'].apply(\n",
    "    lambda tokens: sentence_to_embedding(tokens, word2vec)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Train and Test sets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(reddit_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack embeddings into a single NumPy array\n",
    "sentence_embeddings_train = np.array(train_data['sentence_embedding'].tolist())  \n",
    "sentence_embeddings_test = np.array(test_data['sentence_embedding'].tolist())  \n",
    "\n",
    "# Convert sentence embeddings and labels to tensors\n",
    "X_train = torch.tensor(sentence_embeddings_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(sentence_embeddings_test, dtype=torch.float32).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "labels_train = np.array(train_data['sentiment_label'].tolist())\n",
    "labels_test = np.array(test_data['sentiment_label'].tolist())\n",
    "\n",
    "y_train = torch.tensor(labels_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(labels_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization and split for BERT \n",
    "X_bert_train = bert_tokenize_function(train_data[\"clean_body\"].tolist(), bert_tokenizer)\n",
    "X_bert_test = bert_tokenize_function(test_data[\"clean_body\"].tolist(), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for BERT\n",
    "train_dataset_bert = TensorDataset(\n",
    "    X_bert_train[\"input_ids\"].to(device), \n",
    "    X_bert_train[\"attention_mask\"].to(device), \n",
    "    y_train.to(device)\n",
    ")\n",
    "test_dataset_bert = TensorDataset(\n",
    "    X_bert_test[\"input_ids\"].to(device), \n",
    "    X_bert_test[\"attention_mask\"].to(device), \n",
    "    y_test.to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Word2Vec + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model: 5 layers\n",
    "# # input: vector dim (300,1); sentence vector representation; \n",
    "# # output: 1 output neuron, binary classification \n",
    "\n",
    "# # Define the model\n",
    "# class Word2Vec_NN_binary(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Word2Vec_NN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(300, 128) # Input layer to hidden layer 1\n",
    "#         self.fc2 = nn.Linear(128, 64)  # Hidden layer 1 to hidden layer 2\n",
    "#         self.fc3 = nn.Linear(64, 32)   # Hidden layer 2 to hidden layer 3\n",
    "#         self.fc4 = nn.Linear(32, 16)   # Hidden layer 3 to hidden layer 4\n",
    "#         self.fc5 = nn.Linear(16, 1)    # Hidden layer 4 to output layer\n",
    "#         self.relu = nn.ReLU()          # ReLU activation\n",
    "#         self.sigmoid = nn.Sigmoid()    # Sigmoid activation for binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.relu(self.fc3(x))\n",
    "#         x = self.relu(self.fc4(x))\n",
    "#         x = self.sigmoid(self.fc5(x))  # Output layer\n",
    "#         return x\n",
    "\n",
    "# # Initialize the model\n",
    "# model = Word2Vec_NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: 5 layers\n",
    "# input: vector dim (300,1); sentence vector representation; \n",
    "# output: 3 output neurons, one for each class \n",
    "\n",
    "class W2V_NN_5L_ReLU_MC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(W2V_NN_5L_ReLU_MC, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 128).to(device)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64).to(device)  # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 32).to(device)   # Hidden layer 2 to hidden layer 3\n",
    "        self.fc4 = nn.Linear(32, 16).to(device)   # Hidden layer 3 to hidden layer 4\n",
    "        self.fc5 = nn.Linear(16, 3).to(device)    # Hidden layer 4 to output layer (3 classes)\n",
    "        self.relu = nn.ReLU().to(device)          # ReLU activation\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)  # Softmax activation for multi-class output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.relu(self.fc1(x))  # Pass through first layer with ReLU\n",
    "        x = self.relu(self.fc2(x))  # Pass through second layer with ReLU\n",
    "        x = self.relu(self.fc3(x))  # Pass through third layer with ReLU\n",
    "        x = self.relu(self.fc4(x))  # Pass through fourth layer with ReLU\n",
    "        x = self.fc5(x)             # Output layer (logits)\n",
    "        x = self.softmax(x)         # Softmax activation for probabilities\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = W2V_NN_5L_ReLU_MC().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching\n",
    "dataset = TensorDataset(X.to(device), y.to(device))\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)  # Batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = W2V_NN_5L_ReLU_MC().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CrossEntropyLoss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0  # Track epoch loss\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, labels_tensor = batch  # Get a batch of inputs and labels\n",
    "        inputs = inputs.to(device)\n",
    "        labels_tensor = labels_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels_tensor)  # Compute loss\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "    # Print average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's weights after training\n",
    "torch.save(model.state_dict(), \"W2V_NN_5L_ReLU_MC_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test sentence\n",
    "# test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "test_sentence = \"Bitcoin is going up guys, we will be rich, nice, good\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low, atl, omg\"\n",
    "\n",
    "test_sentence_token_words = clean_tokens(tokenize_function(test_sentence))  # Tokenize and clean\n",
    "test_sentence_embedding = sentence_to_embedding(test_sentence_token_words, word2vec)  # Get embedding\n",
    "\n",
    "# Convert embedding to PyTorch tensor\n",
    "test_sentence_tensor = torch.tensor(test_sentence_embedding, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 300)\n",
    "\n",
    "# Load the trained model\n",
    "model = Word2Vec_NN_multi_class()  # Use the same model architecture\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))  # Load the saved weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Get the model's prediction\n",
    "test_sentence_output = model(test_sentence_tensor)  # Forward pass\n",
    "predicted_class = torch.argmax(test_sentence_output, dim=1).item()  # Get the predicted class index\n",
    "print(f\"Predicted Sentiment: {class_dict[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Word2Vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM pipeline with scaling and the classifier\n",
    "svm_W2V = make_pipeline(\n",
    "    StandardScaler(),  # Standardize features for better performance\n",
    "    SVC(kernel='rbf', \n",
    "        C=1.0, \n",
    "        decision_function_shape='ovr')  # Linear kernel, OvR strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "svm_W2V.fit(sentence_embeddings_train, labels_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test sentence\n",
    "# test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "test_sentence = \"Bitcoin price is increasing, well we have it guys, great job, good nice well perfect\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low, atl, omg\"\n",
    "\n",
    "test_sentence_token_words = clean_tokens(tokenize_function(test_sentence))  # Tokenize and clean\n",
    "test_sentence_embedding = sentence_to_embedding(test_sentence_token_words, word2vec)  # Get embedding\n",
    "\n",
    "\n",
    "# Predict the class\n",
    "predicted_class = svm_model.predict([test_sentence_embedding])[0]\n",
    "print(\"Predicted Class:\", class_dict[predicted_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. BERT + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing plain BERT\n",
    "# text = \"Replace me by any text you'd like.\"\n",
    "# encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_NN_5L_ReLU_MC_DO30(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BERT_NN_5L_ReLU_MC_DO30, self).__init__()\n",
    "        self.bert = bert_model.to(device)  # Pre-trained BERT model\n",
    "        hidden_size = bert_model.config.hidden_size  # Typically 768 for BERT-base\n",
    "\n",
    "        # Define a 5-layer feedforward neural network\n",
    "        self.fc1 = nn.Linear(hidden_size, 512).to(device)  # Layer 1\n",
    "        self.fc2 = nn.Linear(512, 256).to(device)         # Layer 2\n",
    "        self.fc3 = nn.Linear(256, 128).to(device)         # Layer 3\n",
    "        self.fc4 = nn.Linear(128, 64).to(device)          # Layer 4\n",
    "        self.fc5 = nn.Linear(64, num_classes).to(device)  # Output Layer\n",
    "\n",
    "        # Activation function and dropout\n",
    "        self.activation = nn.ReLU().to(device)\n",
    "        self.dropout = nn.Dropout(0.3).to(device)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        with torch.no_grad():  # Freeze BERT weights\n",
    "            outputs = self.bert(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Pass through the 5-layer NN\n",
    "        x = self.dropout(self.activation(self.fc1(pooled_output)))  # Layer 1\n",
    "        x = self.dropout(self.activation(self.fc2(x)))              # Layer 2\n",
    "        x = self.dropout(self.activation(self.fc3(x)))              # Layer 3\n",
    "        x = self.dropout(self.activation(self.fc4(x)))              # Layer 4\n",
    "        logits = self.fc5(x)                                        # Output Layer\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT weights\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize the sentiment analysis model\n",
    "num_classes = 3  # For sentiment analysis (e.g., positive, negative, neutral)\n",
    "sentiment_model = BERT_NN_5L_ReLU_MC_DO30(bert_model, num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(sentiment_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset_bert, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset_bert, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT based model training loop\n",
    "\n",
    "epochs = 5  #10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sentiment_model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, labels in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        # Move data to GPU\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = sentiment_model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {total_loss / len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's weights after training\n",
    "torch.save(sentiment_model.state_dict(), \"BERT_NN_5L_ReLU_MC_DO30_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "text = \"Its acceptable\"\n",
    "encoded_input = bert_tokenize_function(text, bert_tokenizer)\n",
    "\n",
    "# Extract only input_ids and attention_mask\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "# Forward pass through the model\n",
    "sentiment_model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    logits = sentiment_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(logits, dim=1)  # Softmax along class dimension\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n",
    "\n",
    "# Define class labels and convert probabilities to percentages and map to class names\n",
    "class_labels = ['Positive', 'Neutral', 'Negative']\n",
    "probabilities_dict = {class_labels[i]: f\"{probabilities[0, i].item() * 100:.2f} %\" \n",
    "                      for i in range(len(class_labels))}\n",
    "\n",
    "print(probabilities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. BERT + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM pipeline with scaling and the classifier\n",
    "svm_BERT = make_pipeline(\n",
    "    StandardScaler(),  # Standardize features for better performance\n",
    "    SVC(kernel='rbf', \n",
    "        C=1.0, \n",
    "        decision_function_shape='ovr')  # Linear kernel, OvR strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "svm_BERT.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test sentence\n",
    "# test_sentence = \"This is very negative sentence, omg, I hate it\"\n",
    "# test_sentence = \"The price is going down, you will lose all your money\"\n",
    "test_sentence = \"Bitcoin price is increasing, well we have it guys, great job, good nice well perfect\"\n",
    "# test_sentence = \"omg bro this can't be real, we lost, price is very low, atl, omg\"\n",
    "\n",
    "test_sentence_token_words = clean_tokens(tokenize_function(test_sentence))  # Tokenize and clean\n",
    "test_sentence_embedding = sentence_to_embedding(test_sentence_token_words, word2vec)  # Get embedding\n",
    "\n",
    "\n",
    "# Predict the class\n",
    "predicted_class = svm_model.predict([test_sentence_embedding])[0]\n",
    "print(\"Predicted Class:\", class_dict[predicted_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
